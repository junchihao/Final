{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "### GRADED : Boston housing prices\n",
    "We will use the Boston house pricing dataset to predict house prices based on properties of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "boston = sklearn.datasets.load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED\n",
    "# (a) Use statsmodels to build a linear regression model with X_Train, y_train as training data\n",
    "# (b) Generate predictions on X_test and call it yhat\n",
    "# model = smf.OLS(...)\n",
    "# result = model.fit()\n",
    "# yhat = result.predict(...)\n",
    "# Measure the mean square error (https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "# mse_error = np.mean((yhat-y_test) ** 2)\n",
    "# (c) Print the mean squared error\n",
    "# (d) Now, build the same model using sklearn.ensemble.RandomForestRegressor() while using X_train, y_train as training data\n",
    "# (e) Generate mean square error on the test set\n",
    "# (f) Compare the mean square error and determine which regressor is better\n",
    "# model = sklearn.ensemble...\n",
    "# model.fit(...)\n",
    "# yhat = model.predict(...)\n",
    "# mse_error2=np.mean((yhat-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept      36.529642\n",
      "X_train[0]     -0.077785\n",
      "X_train[1]      0.049836\n",
      "X_train[2]      0.056091\n",
      "X_train[3]      3.724686\n",
      "X_train[4]    -19.646002\n",
      "X_train[5]      3.674978\n",
      "X_train[6]      0.007254\n",
      "X_train[7]     -1.491596\n",
      "X_train[8]      0.327521\n",
      "X_train[9]     -0.011057\n",
      "X_train[10]    -0.996877\n",
      "X_train[11]     0.014067\n",
      "X_train[12]    -0.584154\n",
      "dtype: float64\n",
      "[33.29715483 14.26359654 18.74976069 22.67598878 19.89584944 19.57740758\n",
      " 25.97927167 30.37329229 22.70635092 17.72006698 22.77341639 22.16647761\n",
      " 29.98120655 10.52255382 19.31590924 14.22401235 18.58989652 14.9718022\n",
      " 23.81215202 17.36513234 19.58075937 10.00265802 14.12570657 -6.14935911\n",
      " 22.53916315 23.872333   19.66796302 18.62363392 16.02229927 21.39706911\n",
      " 32.17660318 22.12141122 23.15661845 19.48513427 14.93342503 23.61410141\n",
      " 25.1538885  35.35837591 20.63011286 36.76362573 19.1360804  24.99587978\n",
      " 25.53154946 15.50519645 26.97018634 14.79672639 35.23161159 27.7416764\n",
      " 12.05915905 44.68475754 33.79583056 21.94042618 20.27285556 30.09470403\n",
      " 18.89860508 28.54829784 16.53434484 18.69761442 15.65262858 13.38335232\n",
      " 30.31723349 29.30358639 15.21358539 22.91912698 18.64272445 23.42804472\n",
      " 21.98552772 38.01916451 23.72700476 36.5994456  19.9766788  14.70445148\n",
      "  4.08824925 24.04356822 27.60855825 14.13155554 22.0907033   9.88824089\n",
      " 20.84039358 31.56302383 24.5648652  23.49112949 24.72282691 10.20730731\n",
      " 20.43684834 27.37476703  9.65094534 31.27924477 22.61148138 26.18600648\n",
      " 23.36124087 22.10371588  5.78142373 17.68270578 21.40172031 29.52451885\n",
      " 21.29767965 29.15457865 35.92911078 25.0873909  17.09146265 29.56981125\n",
      " 22.1288726  15.04569387 20.67808997 27.45868121 40.27215407 31.96430533\n",
      " 27.4762546  29.29083637 25.29361863 31.20882861 16.74893138 19.52698446\n",
      " 23.12777096 17.71855324 29.95988421 20.42811619 21.35506895 14.78574965\n",
      " 20.9138105  23.59209389 32.94627845 15.62463587  8.97955123 29.02510426\n",
      " 22.14908403 31.001287   14.63658828 20.03751233 21.74023903 21.32348823\n",
      " 16.15154391 15.81663057  9.10466683 20.90594006 25.42589958 22.91741499\n",
      "  8.35353699 -3.42993635 32.98633185 21.09877989 32.25822867 23.42334395\n",
      " 23.29870298 17.24286045 30.81184825 12.61878678 21.41775865  4.30017669\n",
      " 16.03109105 14.36300652]\n",
      "23.974715634680077\n"
     ]
    }
   ],
   "source": [
    "#(a)\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols('y_train~X_train',data=boston)\n",
    "result=model.fit()\n",
    "print(result.params)\n",
    "result.summary()\n",
    "#(b)\n",
    "model = smf.OLS(y_train,X_train)\n",
    "result=model.fit()\n",
    "yhat=result.predict(X_test)\n",
    "print(yhat)\n",
    "#(c)\n",
    "mse_error = np.mean((yhat-y_test) ** 2)\n",
    "print(mse_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34.48 20.68 17.53 21.18 21.3  25.63 25.08 29.81 28.69 18.93 20.88 17.24\n",
      " 26.1  14.96 19.56 12.31 12.49 18.97 19.54 19.84 25.9  18.66 15.65  9.3\n",
      " 24.42 21.14 15.2  15.64 16.25 21.28 30.49 22.03 21.35 12.46 19.3  22.17\n",
      " 23.75 34.21 21.44 41.26 19.52 24.6  20.83 19.94 21.9  12.52 38.77 23.99\n",
      " 14.   48.54 34.65 21.71 14.33 26.73 13.85 26.69 15.74 22.06 16.36 19.43\n",
      " 31.07 30.37 20.22 20.09 19.35 21.9  23.08 44.31 21.26 39.88 22.42 13.7\n",
      "  9.73 22.08 27.21 12.34 23.02 14.02 20.44 27.98 22.48 14.45 21.75 10.15\n",
      " 20.25 25.9  14.03 32.55 15.93 23.77 21.79 21.85 16.21 18.62 19.63 29.32\n",
      " 19.96 27.9  42.26 24.53 20.01 26.84 21.12 14.52 20.05 23.12 41.7  34.55\n",
      " 23.16 21.89 21.29 26.85 16.39 19.47 26.56 16.43 27.17 19.57 20.32 15.18\n",
      " 20.89 22.   34.49 15.37  7.94 27.25 19.74 24.   22.54 20.17 17.24 20.65\n",
      " 17.14 15.53  9.19 17.34 23.71 15.12 13.44 12.   32.25 14.36 26.59 18.68\n",
      " 21.47 21.36 31.95 39.98 25.63  8.78 19.22 11.82]\n",
      "16.333780921052632\n",
      "7.640934713627445\n",
      "sklearn.ensemble.RandomForestRegressor is better\n"
     ]
    }
   ],
   "source": [
    "#(d)\n",
    "from patsy import dmatrices\n",
    "y,X= dmatrices('y_train~X_train-y_train',data=boston)\n",
    "model = sklearn.ensemble.RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "yhat=model.predict(X_test)\n",
    "print(yhat)\n",
    "#(e)\n",
    "mse_error2=np.mean((yhat-y_test)**2)\n",
    "print(mse_error2)\n",
    "#(f)\n",
    "print(mse_error-mse_error2)  # result>0,  mse_error>mse_error2, sklearn.ensemble.RandomForestRegressor is better\n",
    "print('sklearn.ensemble.RandomForestRegressor is better')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded : Iris classification\n",
    "In this example, we will use the familiar Iris dataset and build a classifier to predict if a leaf is of species 'setosa' based on sepal and petal measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333334\n"
     ]
    }
   ],
   "source": [
    "# Get the features and target variables\n",
    "X=iris.data\n",
    "y=(iris.target==0).astype(np.float32) # We convert a 3 class problem to is_setosa/is_not_setosa\n",
    "print(np.mean(y))\n",
    "# We will split the dataset into 70% training and 30% test dataset\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED\n",
    "# (a) Use sklearn to build a classification model with X_Train, y_train as training data\n",
    "# (b) Generate predictions on X_test and call it yhat\n",
    "# model = sklearn.ensemble.RandomForestClassifier(n_estimators=1)\n",
    "# result = model.fit(...)\n",
    "# yhat_test = result.predict(X_test)\n",
    "# yhat_train = ...\n",
    "# (c) Measure the average precision and recall on the trianing set and the test set\n",
    "# sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        30\n",
      "        1.0       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        70\n",
      "        1.0       1.00      1.00      1.00        35\n",
      "\n",
      "avg / total       1.00      1.00      1.00       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(a)\n",
    "model = sklearn.ensemble.RandomForestClassifier(n_estimators=1)\n",
    "result=model.fit(X_train,y_train)\n",
    "print(result)\n",
    "#(b)\n",
    "yhat_test = result.predict(X_test)\n",
    "yhat_train = result.predict(X_train)\n",
    "print(yhat_test)\n",
    "print(yhat_train)\n",
    "#(c)\n",
    "print(sklearn.metrics.classification_report(y_test, yhat_test))   #test set\n",
    "print(sklearn.metrics.classification_report(y_train, yhat_train)) #train set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT GRADED: Capstone project\n",
    "We will continue working on our capstone project this week. For this week, the task consists of\n",
    "* Seeing if you can predict trends in your data\n",
    "* Use either regerssion or classification models to do the prediction and quantify the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
